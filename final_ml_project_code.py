# -*- coding: utf-8 -*-
"""Final_ML_Project_Code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RCKfHG0VOc7l3D3BaphkBaChefKJdnhQ
"""

# !pip install viz
# !pip install scipy==1.10.1
# !apt-get install -y sox libsox-dev libsox-fmt-all
# !pip install --force-reinstall nussl
# !pip uninstall numpy
# !pip install numpy==1.24.3
# !pip uninstall scipy
# !pip install scipy==1.10.1

# import nussl
# print(nussl.__version__)

import numpy as np
print(np.__version__)

import matplotlib.pyplot as plt
import librosa
from pprint import pprint
import viz

# import os
# # Mount Google Drive
# from google.colab import drive
# drive.mount('/content/drive')

# # Read file
# folder_path = '/content/drive/My Drive/MLProject/'
# for file_name in os.listdir(folder_path):
#   file_path = os.path.join(folder_path, file_name)
#   signal = nussl.AudioSignal(file_path)
#   print(signal)

!pip install Gradio

import numpy as np
import librosa
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from scipy.signal import istft
import matplotlib.pyplot as plt
import os
import nussl
import types
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd
import viz
import soundfile as sf
from scipy.special import rel_entr
import gradio as gr
import pandas as pd
import types
import soundfile as sf
from io import BytesIO


def save_audio_files(bg,fg, model):


  # Path to the input audio file
  folder_path_bg = '/content/drive/My Drive/Background_audio'
  folder_path_fg = '/content/drive/My Drive/Foreground_audio'


  # Access the audio data from the AudioSignal objects
  bg_audio = bg.audio_data
  fg_audio = fg.audio_data
  sr = bg.sample_rate  # Assuming both signals have the same sample rate

  # Ensure the audio data is in the expected format (e.g., NumPy array, float32)
  bg_audio = bg_audio.astype('float32')  # Convert to float32 if necessary
  fg_audio = fg_audio.astype('float32')  # Convert to float32 if necessary

  # Save the separated background and foreground audio to new files
  # Explicitly specifying the format as 'WAV'
  sf.write(os.path.join(folder_path_bg, 'bg_audio'+model+'.wav'), bg_audio.T, sr, format='WAV')
  sf.write(os.path.join(folder_path_fg, 'fg_audio'+model+'.wav'), fg_audio.T, sr, format='WAV')

  print("Background and foreground audio saved successfully.")


def calculate_psnr(clean_signal, separated_signal):
    """
    Calculate the Peak Signal-to-Noise Ratio (PSNR) between a clean signal and a separated signal.

    Args:
        clean_signal (numpy.ndarray): The ground truth clean signal.
        separated_signal (numpy.ndarray): The source-separated signal.

    Returns:
        float: PSNR value in dB.
    """
    # Ensure both signals have the same length
    min_length = min(len(clean_signal), len(separated_signal))
    clean_signal = clean_signal[:min_length]
    separated_signal = separated_signal[:min_length]

    # Calculate Mean Squared Error (MSE)
    mse = np.mean((clean_signal - separated_signal) ** 2)

    if mse == 0:
        return float('inf')  # Infinite PSNR if the signals are identical

    # Calculate maximum possible signal value (assuming normalized signals)
    max_val = np.max(np.abs(clean_signal))

    # Calculate PSNR
    psnr = 10 * np.log10((max_val ** 2) / mse)
    return psnr


def calculate_snr(original_signal, separated_signal):
    # Ensure the signals have the same length
    min_length = min(len(original_signal), len(separated_signal))
    original_signal = original_signal[:min_length]
    separated_signal = separated_signal[:min_length]
    # print(min_length, len(original_signal),len(separated_signal))
    power_original = np.sum(original_signal**2)

    # Compute the power of the noise (difference between original and separated)
    noise = original_signal - separated_signal

    power_noise = np.sum(noise**2)

    # Calculate SNR in dB
    snr = 10 * np.log10(power_original / power_noise)

    return snr

def compute_mel_spectrogram(signal, sr, n_fft=2048, hop_length=512, n_mels=128):
    """
    Computes the Mel-spectrogram of a given signal.

    Args:
        signal (numpy.ndarray): The input audio signal.
        sr (int): The sampling rate of the audio signal.
        n_fft (int): Number of FFT components.
        hop_length (int): Hop length for STFT.
        n_mels (int): Number of Mel bands.

    Returns:
        numpy.ndarray: Mel-spectrogram (power).
    """
    mel_spectrogram = librosa.feature.melspectrogram(
        y=signal, sr=sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels, power=2.0
    )
    return mel_spectrogram

def kl_divergence(p, q):
    """
    Computes the KL Divergence between two probability distributions.

    Args:
        p (numpy.ndarray): First probability distribution.
        q (numpy.ndarray): Second probability distribution.

    Returns:
        float: KL divergence value.
    """
    # Avoid division by zero and log(0) issues by adding a small epsilon
    epsilon = 1e-10
    p = np.clip(p, epsilon, None)
    q = np.clip(q, epsilon, None)
    return np.sum(rel_entr(p, q))

def calculate_melspectrogram_kl_divergence(clean_signal, separated_signal, sr):
    """
    Computes the KL divergence between the Mel-spectrograms of the clean signal and the separated signal.

    Args:
        clean_signal (numpy.ndarray): Ground truth clean signal.
        separated_signal (numpy.ndarray): Source-separated signal.
        sr (int): Sampling rate of the signals.

    Returns:
        float: KL divergence value between the two Mel-spectrograms.
    """
    # Compute Mel-spectrograms
    clean_mel = compute_mel_spectrogram(clean_signal, sr)
    separated_mel = compute_mel_spectrogram(separated_signal, sr)

    # Normalize Mel-spectrograms to create probability distributions
    clean_mel_norm = clean_mel / np.sum(clean_mel)
    separated_mel_norm = separated_mel / np.sum(separated_mel)

    # Calculate KL divergence
    kl_div = kl_divergence(clean_mel_norm, separated_mel_norm)
    return kl_div

def _validate_mask_patched(self, mask_): # Added 'self' as the first argument
    assert isinstance(mask_, np.ndarray), 'Mask must be a numpy array!'

    # Change: Replace np.bool with bool
    if mask_.dtype == bool:
        # This is perfect, do nothing here
        return mask_
    # If mask values are not in {0, 1},
    # they will be thresholded to {0, 1}
    mask_ = mask_ > 0.5  # type: ignore
    if not np.all(np.logical_or(mask_, np.logical_not(mask_))):
        raise ValueError('All mask entries must be 0 or 1.')
    return mask_

def visualize_spectogram(signal, mix):
    # Listen to the audio
    # print(signal)
    mix.embed_audio()
    # Visualize the spectrogram
    plt.figure(figsize=(10, 3))
    plt.title('Mixture spectrogram')

    audio_data = mix.get_channel(0)  # Get the first channel
    mel_spectrogram = librosa.feature.melspectrogram(y=audio_data, sr=mix.sample_rate)

    # Visualize mel spectrogram manually
    librosa.display.specshow(librosa.power_to_db(mel_spectrogram, ref=np.max), x_axis='time', y_axis='mel')
    plt.colorbar(format='%+2.0f dB')

    plt.tight_layout()
    plt.show()

def Repet(mix):
    repet = nussl.separation.primitive.Repet(mix,min_period=0.05)
    repet_bg, repet_fg = repet()
    return(repet_bg, repet_fg)

def Repet_Sim(mix):

    repet_sim = nussl.separation.primitive.RepetSim(mix)
    rsim_bg, rsim_fg = repet_sim()
    return rsim_bg, rsim_fg

def Two_DFT(mix):
    ft2d = nussl.separation.primitive.FT2D(mix)
    ft2d_bg, ft2d_fg = ft2d()
    return ft2d_bg, ft2d_fg

def visualize_snr_for_all_models(data,file_path,metric):

    plt.figure(figsize=(8, 6))
    sns.barplot(x='Type', y=metric, hue='Model', data=data, palette='Set2')

    start_index = file_path.find("id")
    end_index = file_path.find(".wav") + 4
    extracted_part = file_path[start_index:end_index]

    # Add labels and title
    plt.xlabel('Audio Signals', fontsize=12)
    if metric == 'PSNR':
      plt.ylabel('PSNR (Peak Signal-to-Noise Ratio)', fontsize=12)
      plt.title(f"PSNR Values - {extracted_part}", fontsize=14)
      plt.savefig(f"PSNR Values - {extracted_part}.png", dpi=300, format="png")

    elif metric == 'KLDIV':
      plt.ylabel('KL Divergence', fontsize=12)
      plt.title(f"KL Divergence Values - {extracted_part}", fontsize=14)
      plt.savefig(f"KL Divergence Values - {extracted_part}.png", dpi=300, format="png")

    else:
      plt.ylabel('SNR (Signal-to-Noise Ratio)', fontsize=12)
      plt.title(f"SNR Values - {extracted_part}", fontsize=14)
      plt.savefig(f"SNR Values - {extracted_part}.png", dpi=300, format="png")

    # Display grid for clarity
    plt.grid(axis='y', alpha=0.3)

    # Show the plot
    # plt.show()


# Helper function to extract features
def extract_features(audio, sr, frame_size=5046, hop_length=2048):
    """Extracts audio features for classification."""
    # Time-domain features
    zcr = librosa.feature.zero_crossing_rate(audio, frame_length=frame_size, hop_length=hop_length)
    rms = librosa.feature.rms(y=audio, frame_length=frame_size, hop_length=hop_length)


    # Frequency-domain features
    # mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13, hop_length=hop_length)
    spectral_centroid = librosa.feature.spectral_centroid(y=audio, sr=sr, hop_length=hop_length)
    spectral_flux = librosa.onset.onset_strength(y=audio, sr=sr, hop_length=hop_length)

    # Concatenate features
    features = np.vstack((zcr, rms, spectral_centroid)).T  # Transpose for frame-wise representation
    hfeatures = np.hstack((zcr, rms, spectral_centroid))
    return hfeatures


# Load audio files
def load_audio(file_path, sr=22050):
    """Loads an audio file."""
    audio, sr = librosa.load(file_path, sr=sr, mono=True)
    return audio, sr


# Reconstruct audio from predicted labels
def reconstruct_audio(mixed_audio, labels, sr, frame_size=2048, hop_length=512):
    """Reconstructs the separated audio signals using labels."""
    # Reshape the mixed audio into frames
    frames = librosa.util.frame(mixed_audio, frame_length=frame_size, hop_length=hop_length).T

    # Separate foreground and background frames
    num_frames = frames.shape[0]
    labels = labels[:num_frames]

    fg_frames = frames[labels == 1.0]  # Foreground
    bg_frames = frames[labels == 0.0]  # Background
    # Reconstruct audio from frames
    if len(fg_frames) != 0:
      fg_audio = librosa.istft(fg_frames.T, hop_length=hop_length)
    else:
      fg_audio = np.zeros_like(mixed_audio)
      # fg_audio = librosa.istft(fg_frames.T, hop_length=hop_length)
    if len(bg_frames) != 0:
      bg_audio = librosa.istft(bg_frames.T, hop_length=hop_length)
    else:
      bg_audio = np.zeros_like(mixed_audio)
      # bg_audio = librosa.istft(bg_frames.T, hop_length=hop_length)

    return fg_audio, bg_audio


# Main processing pipeline
def process_pipeline(fg_path, bg_path, df_bgfg, df_labels):
    """Processes audio files for foreground/background separation."""
    # Load audio files
    fg_audio, sr = load_audio(fg_path)
    bg_audio, _ = load_audio(bg_path)
    # mixed_audio, _ = load_audio(mixed_path)

    # Extract features
    fg_features = extract_features(fg_audio, sr)
    bg_features = extract_features(bg_audio, sr)




    # mixed_features = extract_features(mixed_audio, sr)

    # print(fg_features.shape, bg_features.shape, len(fg_features))

    # Create labels
    fg_labels = np.ones(fg_features.shape[0])  # Foreground = 1
    bg_labels = np.zeros(bg_features.shape[0])  # Background = 0

    # Combine features and labels for training
    features = np.vstack((fg_features, bg_features))
    labels = np.vstack((fg_labels, bg_labels))
    df_bgfg = pd.concat((df_bgfg.copy(), pd.DataFrame(features))).reset_index(drop=True)
    df_labels = pd.concat((df_labels.copy(), pd.DataFrame(labels))).reset_index(drop=True)

    return df_bgfg, df_labels

def train_model(df_fgbg):
    # Split data into train and test sets
    X_train, X_val, y_train, y_val = train_test_split(df_fgbg.values[:,:-1],
                                                      df_fgbg.values[:,-1],
                                                      test_size=0.2,
                                                      random_state=42,
                                                      shuffle=True)

    # Train a Random Forest classifier
    clf = RandomForestClassifier(n_estimators=100, random_state=42)
    clf.fit(X_train, y_train)
    print(X_train, y_train)
    # Evaluate the classifier
    y_pred = clf.predict(X_val)
    # print("Classification Report:")
    print(classification_report(y_val, y_pred))
    return clf
    #
def test_model(model, test_file_path):
    mixed_audio, sr = load_audio(test_file_path)

    # Extract features
    mixed_features = extract_features(mixed_audio, sr)


    #  Predict on mixed audio
    predicted_labels = model.predict(mixed_features)
    print(predicted_labels)

    # Reconstruct FG and BG audio
    fg_audio_reconstructed, bg_audio_reconstructed = reconstruct_audio(mixed_audio, predicted_labels, sr)

    # Save outputs
    folder_path = '/content/drive/My Drive/MLProject/'
    sf.write(test_file_path[:-4]+'reconstructed_fg.wav', fg_audio_reconstructed, sr, format='WAV') # Changed to soundfile.write
    sf.write(test_file_path[:-4]+'reconstructed_bg.wav', bg_audio_reconstructed, sr, format='WAV') # Changed to soundfile.write

    print("Reconstruction completed. Files saved as", test_file_path[:-4]+'reconstructed_fg.wav and' ,test_file_path[:-4]+'reconstructed_bg.wav')
    # return fg_audio_reconstructed, bg_audio_reconstructed

def process_audio(file_path):
    signal = nussl.AudioSignal(file_path)
    mix_signal, sr1 = librosa.load(file_path, sr=None)

    print('Processing with 2DFT Model...')
    ft2d_bg, ft2d_fg = Two_DFT(signal)

    print('Processing with Repet Model...')
    repet_bg, repet_fg = Repet(signal)

    print('Processing with Repet_Sim Model...')
    rsim_bg, rsim_fg = Repet_Sim(signal)

    output_signal = rsim_fg

    output_file1 = 'foreground_output1.wav'
    output_file2 = 'foreground_output2.wav'
    output_file3 = 'foreground_output3.wav'
    # output_signal.write_audio_to_file(output_file)
    ft2d_fg.write_audio_to_file(output_file1)
    repet_fg.write_audio_to_file(output_file2)
    rsim_fg.write_audio_to_file(output_file3)

    output_snr1 = calculate_psnr(signal.audio_data, ft2d_fg.audio_data)
    output_snr2 = calculate_psnr(signal.audio_data, repet_fg.audio_data)
    output_snr3 = calculate_psnr(signal.audio_data, rsim_fg.audio_data)
    output_kl1 = calculate_melspectrogram_kl_divergence(signal.audio_data, ft2d_fg.audio_data, sr1)
    output_kl2 = calculate_melspectrogram_kl_divergence(signal.audio_data, repet_fg.audio_data, sr1)
    output_kl3 = calculate_melspectrogram_kl_divergence(signal.audio_data, rsim_fg.audio_data, sr1)

    return output_file1, output_snr1, output_kl1, output_file2, output_snr2, output_kl2, output_file3, output_snr3, output_kl3

# Read file
folder_path = '/content/drive/My Drive/MLProject/'
# Apply the patch
nussl.core.masks.binary_mask.BinaryMask._validate_mask = types.MethodType(_validate_mask_patched, nussl.core.masks.binary_mask.BinaryMask)

count = 0
df_fgbg = pd.DataFrame({})
df_labels = pd.DataFrame({})

for file_name in os.listdir(folder_path):

    if file_name.endswith('.wav') and count<20:
        file_path = os.path.join(folder_path, file_name)
        count=count+1
    else:
        df_fgbg = df_fgbg.fillna(0.0)
        df_fgbg = pd.concat((df_fgbg, df_labels), axis=1)
        trained_model = train_model(df_fgbg)
        file_path_test = os.path.join(folder_path, file_name)
        # test_model(trained_model,file_path_test)
        break

    signal = nussl.AudioSignal(file_path)
    mix_signal, sr1 = librosa.load(file_path, sr=None)

    # Get the mix and sources
    mix = signal

    visualize_spectogram(signal, mix)

    #Apply REPET Model
    print('Repet_Model')
    repet_bg, repet_fg = Repet(mix)
    nussl.play_utils.multitrack({'Background': repet_bg, 'Foreground': repet_fg})

    #Apply REPET_Sim Model
    print('Repet_Sim Model')
    rsim_bg, rsim_fg = Repet_Sim(mix)
    nussl.play_utils.multitrack({'Background': rsim_bg, 'Foreground': rsim_fg})

    #Apply Two-dimensional Fourier Transform (2DFT)
    print('2DFT Model')
    ft2d_bg, ft2d_fg = Two_DFT(mix)
    nussl.play_utils.multitrack({'Background': ft2d_bg, 'Foreground': ft2d_fg})

    #Calcultae SNR_Repet
    snr_repet = calculate_snr(repet_fg.audio_data, repet_bg.audio_data)
    bsnr_repet=calculate_snr(mix.audio_data, repet_bg.audio_data)
    fsnr_repet=calculate_snr(mix.audio_data, repet_fg.audio_data)

    #Calculate SNR_Repet_SIM
    snr_repet_sim = calculate_snr(rsim_fg.audio_data, rsim_bg.audio_data)

    bsnr_repet_sim = calculate_snr(mix.audio_data, rsim_bg.audio_data)
    fsnr_repet_sim = calculate_snr(mix.audio_data, rsim_fg.audio_data)

    #Calculate SNR_2DFT
    snr_2dft = calculate_snr(ft2d_fg.audio_data, ft2d_bg.audio_data)
    bsnr_2dft = calculate_snr(mix.audio_data, ft2d_bg.audio_data)
    fsnr_2dft = calculate_snr(mix.audio_data, ft2d_fg.audio_data)

    #Calculate PSNR_Repet
    bpsnr_repet=calculate_psnr(mix.audio_data, repet_bg.audio_data)
    fpsnr_repet=calculate_psnr(mix.audio_data, repet_fg.audio_data)

    #Calculate PSNR_Repet_SIM
    bpsnr_repet_sim = calculate_psnr(mix.audio_data, rsim_bg.audio_data)
    fpsnr_repet_sim = calculate_psnr(mix.audio_data, rsim_fg.audio_data)

    #Calculate PSNR_2DFT
    bpsnr_2dft = calculate_psnr(mix.audio_data, ft2d_bg.audio_data)
    fpsnr_2dft = calculate_psnr(mix.audio_data, ft2d_fg.audio_data)

    #Calculate KL_divergence_Repet
    kl_div_value_repet_bg = calculate_melspectrogram_kl_divergence(mix.audio_data, repet_bg.audio_data, sr1)
    kl_div_value_sim_bg = calculate_melspectrogram_kl_divergence(mix.audio_data, rsim_bg.audio_data, sr1)
    kl_div_value_2dft_bg = calculate_melspectrogram_kl_divergence(mix.audio_data, ft2d_bg.audio_data, sr1)
    kl_div_value_repet_fg = calculate_melspectrogram_kl_divergence(mix.audio_data, repet_fg.audio_data, sr1)
    kl_div_value_sim_fg = calculate_melspectrogram_kl_divergence(mix.audio_data, rsim_fg.audio_data, sr1)
    kl_div_value_2dft_fg = calculate_melspectrogram_kl_divergence(mix.audio_data, ft2d_fg.audio_data, sr1)


    # Example data: SNR values for 3 models
    models = ['repet', 'ft2d', 'repet_SIM']
    snr_values = [snr_repet, snr_2dft, snr_repet_sim]
    bsnr_values = [bsnr_repet, bsnr_2dft,bsnr_repet_sim]  # Background SNR for each model
    fsnr_values = [fsnr_repet, fsnr_2dft, fsnr_repet_sim]  # Foreground SNR for each model

    print('SNR Values for Background Audio:-',{m:bsnr for m,bsnr in list(zip(models,bsnr_values))})
    print('SNR Values for Foreground Audio:-',{m:fsnr for m,fsnr in list(zip(models,fsnr_values))})

    # Create a DataFrame for Seaborn
    data = pd.DataFrame({
    'Model': models * 2,  # Repeat the model names for BSNR and FSNR
    'SNR': bsnr_values + fsnr_values,  # Combine BSNR and FSNR values
    'Type': ['Background Audio'] * len(models) + ['Foreground Audio'] * len(models)  # Label for the type of SNR
    })

    data_snr = pd.DataFrame({
    'Model': models,  # Repeat the model names for BSNR and FSNR
    'SNR': snr_values ,  # Combine BSNR and FSNR values
    'Type': ['Audio'] * len(models)  # Label for the type of SNR
    })


    bpsnr_values = [bpsnr_repet, bpsnr_2dft, bpsnr_repet_sim ]  # Background PSNR for each model
    fpsnr_values = [fpsnr_repet, fpsnr_2dft, fpsnr_repet_sim ]  # Foreground PSNR for each model

    print('PSNR Values for Background Audio:-',{m:bpsnr for m,bpsnr in list(zip(models,bpsnr_values))})
    print('PSNR Values for Foreground Audio:-',{m:fpsnr for m,fpsnr in list(zip(models,fpsnr_values))})

    # Create a DataFrame for Seaborn
    data_psnr = pd.DataFrame({
    'Model': models * 2,  # Repeat the model names for BSNR and FSNR
    'PSNR': bpsnr_values + fpsnr_values,  # Combine BSNR and FSNR values
    'Type': ['Background Audio'] * len(models) + ['Foreground Audio'] * len(models)  # Label for the type of SNR
    })


    # visualize_snr_for_all_models(data_psnr, file_path, 'PSNR')

    bkldiv_values = [kl_div_value_repet_bg, kl_div_value_2dft_bg,kl_div_value_sim_bg ]  # Background KLDIV for each model
    fkldiv_values = [kl_div_value_repet_fg, kl_div_value_2dft_fg, kl_div_value_sim_fg ]  # Foreground KLDIV for each model

    print('KLDivergence Values for Background Audio:-',{m:bkldiv for m,bkldiv in list(zip(models,bkldiv_values))})
    print('KLDivergence Values for Foreground Audio:-',{m:fkldiv for m,fkldiv in list(zip(models,fkldiv_values))})

    # Create a DataFrame for Seaborn
    data_kldiv = pd.DataFrame({
    'Model': models * 2,  # Repeat the model names for BSNR and FSNR
    'KLDIV': bkldiv_values + fkldiv_values,  # Combine BSNR and FSNR values
    'Type': ['Background Audio'] * len(models) + ['Foreground Audio'] * len(models)  # Label for the type of SNR
    })

    # visualize_snr_for_all_models(data_kldiv, file_path, 'KLDIV')

    save_audio_files(repet_bg, repet_fg, 'repet')
    save_audio_files(rsim_bg, rsim_fg, 'rsim')
    save_audio_files(ft2d_bg, ft2d_fg, 'ft2d')

    #Select the best model out of Repet, Repet_Sim, 2DFT
    indexes = (np.argmax(fsnr_values) , np.argmax(fpsnr_values), np.argmax(fkldiv_values))
    print(indexes)
    for ind in indexes:
        if indexes.count(ind) == 3:
            model_to_be_used = models[ind]
            break
        elif indexes.count(ind) == 2:
            model_to_be_used = models[ind]
            break
    else:
        model_to_be_used = 'repet'

    folder_path_bg = '/content/drive/My Drive/Background_audio'
    folder_path_fg = '/content/drive/My Drive/Foreground_audio'
    dict_to_separate_signal={}
    for model in models:
        dict_to_separate_signal[model] = (os.path.join(folder_path_bg, 'bg_audio'+model+'.wav'), os.path.join(folder_path_fg, 'fg_audio'+model+'.wav'))
    fg_path, bg_path = dict_to_separate_signal[model_to_be_used]


    df_fgbg, df_labels = process_pipeline(fg_path, bg_path, df_fgbg, df_labels)
    print(df_fgbg.shape)

#TESTING ENVIRONMENT
interface = gr.Interface(
    fn=process_audio,
    inputs=gr.Audio(type="filepath", label="Upload Audio File"),
    outputs=[
        gr.Audio(label="Foreground Audio 2DFT"),
        gr.Textbox(label="PSNR (dB) 2DFT", interactive = False),
        gr.Textbox(label="KL Divergence 2DFT", interactive = False),
        gr.Audio(label="Foreground Audio Repet"),
        gr.Textbox(label="PSNR (dB) Repet", interactive = False),
        gr.Textbox(label="KL Divergence Repet", interactive = False),
        gr.Audio(label="Foreground Audio Repet Sim"),
        gr.Textbox(label="PSNR (dB) Repet Sim", interactive = False),
        gr.Textbox(label="KL Divergence Repet Sim", interactive = False)
    ],#, gr.Text(label="PSNR"), gr.Image(label="KLdivergence")],
    title="Audio Foreground Separator",
    description="Upload an audio file to separate its foreground using 2DFT, Repet, and Repet_Sim models."
)

interface.launch()

output_file1, output_snr1, output_kl1, output_file2, output_snr2, output_kl2, output_file3, output_snr3, output_kl3 = process_audio('/content/drive/MyDrive/testaudio.wav')
print(f"2DFT PSNR: {output_snr1}  and KL Divergence: {output_kl1}")
print(f"Repet PSNR: {output_snr2}  and KL Divergence: {output_kl2}")
print(f"Repet Sim PSNR: {output_snr3}  and KL Divergence: {output_kl3}")

signal_2dft = nussl.AudioSignal(output_file1)
signal_repet = nussl.AudioSignal(output_file2)
signal_rsim = nussl.AudioSignal(output_file3)

nussl.play_utils.multitrack({
    '2DFT': signal_2dft,
    'Repet': signal_repet,
    'Repet Sim': signal_rsim
})

